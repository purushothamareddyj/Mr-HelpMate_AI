{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUA5JZ5IeOuq"
      },
      "source": [
        "# Interactive Insurance Query-Answering System\n",
        "\n",
        "## Overview\n",
        "This notebook demonstrates an interactive Question-Answering (QA) system for insurance policies using a Retrieval-Augmented Generation (RAG) pipeline. The system integrates semantic search, document embeddings, caching, cross-encoder reranking, and GPT-based response generation.\n",
        "\n",
        "## Features\n",
        "- Extracts and processes text, including tables, from insurance policy documents (or generic documents of your choice with minor tweaks).\n",
        "- Generates semantic embeddings for text retrieval.\n",
        "- Implements caching for efficient query handling.\n",
        "- Reranks results using a cross-encoder for higher accuracy.\n",
        "- Synthesizes detailed responses using a model of your choice.\n",
        "- Interactive query inputs and visualizations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGngXRTZecBY"
      },
      "source": [
        "## 1. Setup and Configuration\n",
        "\n",
        "### User-Defined Variables\n",
        "Configure these before running the notebook.\n",
        "\n",
        "Your OpenAI API keys are found here - https://platform.openai.com/api-keys\n",
        "\n",
        "OpenAI model details are available here - https://platform.openai.com/docs/models#gpt-4o-mini\n",
        "\n",
        "A sample set of insurance policy documents are available here - https://www.hdfclife.com/policy-documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJanBCgnb3dG"
      },
      "outputs": [],
      "source": [
        "DOCUMENT_LOCATION = \"/content/drive/MyDrive/rag_test/policy_documents\"  # Path to insurance PDFs\n",
        "OPENAI_API_KEY = \"your_key_here\"  # Replace with your OpenAI API key\n",
        "MODEL_NAME = \"gpt-4o-mini\"  # GPT model for response generation\n",
        "CHROMA_PATH = \"/content/drive/MyDrive/rag_test/chromadb\"  # Path for ChromaDB storage, can be an empty folder on your GDrive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JDI7lFDmJHv"
      },
      "source": [
        "### Mounting GDrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LL8IcwkimOvT"
      },
      "outputs": [],
      "source": [
        "# Comment out below section if running locally\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4xKGxZGeixW"
      },
      "source": [
        "### Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ZEAYep2ejwT"
      },
      "outputs": [],
      "source": [
        "!pip install openai chromadb pdfplumber tiktoken sentence-transformers -q\n",
        "# Replace ! with % if using Jupyter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pfr97kXzesDT"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "import pdfplumber\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "from chromadb import PersistentClient\n",
        "from chromadb.utils.embedding_functions import OpenAIEmbeddingFunction, SentenceTransformerEmbeddingFunction\n",
        "from sentence_transformers import CrossEncoder\n",
        "from IPython.display import display\n",
        "import ipywidgets as widgets\n",
        "import numpy as np\n",
        "import time\n",
        "import ast\n",
        "openai.api_key = OPENAI_API_KEY"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPOtNxKyemXH"
      },
      "source": [
        "## 2. Data Preparation\n",
        "Processes and filters the extracted text from insurance policy PDFs for further analysis.\n",
        "### PDF Extraction and Metadata Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y0oyiuhqeze9"
      },
      "outputs": [],
      "source": [
        "def check_boxes(word, table_box):\n",
        "    l = word['x0'], word['top'], word['x1'], word['bottom']\n",
        "    r = table_box\n",
        "    return l[0] > r[0] and l[1] > r[1] and l[2] < r[2] and l[3] < r[3]\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    \"\"\"\n",
        "    Extract text and tables from PDF documents.\n",
        "\n",
        "    Parameters:\n",
        "    - pdf_path (str): Path to the PDF file.\n",
        "\n",
        "    Returns:\n",
        "    - list: Extracted text with metadata.\n",
        "    \"\"\"\n",
        "    full_text = []\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        for p, page in enumerate(pdf.pages):\n",
        "            page_no = f\"Page {p+1}\"\n",
        "            text = page.extract_text()\n",
        "            tables = page.find_tables()\n",
        "            table_boxes = [t.bbox for t in tables]\n",
        "            non_table_words = [\n",
        "                word for word in page.extract_words()\n",
        "                if not any(check_boxes(word, box) for box in table_boxes)\n",
        "            ]\n",
        "            lines = [word['text'] for word in non_table_words]\n",
        "            full_text.append([page_no, \" \".join(lines)])\n",
        "            # Debugging Output\n",
        "            print(f\"Page {p+1}: Extracted {len(lines)} lines of text.\")\n",
        "    return full_text\n",
        "\n",
        "pdf_dir = Path(DOCUMENT_LOCATION)\n",
        "data = []\n",
        "for pdf_path in pdf_dir.glob(\"*.pdf\"):\n",
        "    print(f\"Processing {pdf_path.name}...\")\n",
        "    extracted = extract_text_from_pdf(pdf_path)\n",
        "    df = pd.DataFrame(extracted, columns=['Page No.', 'Page_Text'])\n",
        "    df['Document_Name'] = pdf_path.name\n",
        "    data.append(df)\n",
        "insurance_pdfs_data = pd.concat(data, ignore_index=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r07A5Qtxe6a0"
      },
      "source": [
        "### Metadata and Filtering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ODG9IUPme7Fy"
      },
      "outputs": [],
      "source": [
        "insurance_pdfs_data['Text_Length'] = insurance_pdfs_data['Page_Text'].apply(lambda x: len(x.split()))\n",
        "insurance_pdfs_data = insurance_pdfs_data[insurance_pdfs_data['Text_Length'] >= 10]\n",
        "insurance_pdfs_data['Metadata'] = insurance_pdfs_data.apply(\n",
        "    lambda x: {\"Policy_Name\": x['Document_Name'], \"Page_No\": x['Page No.']}, axis=1\n",
        ")\n",
        "print(\"Final processed DataFrame:\")\n",
        "print(insurance_pdfs_data.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yuOYmrjce-vL"
      },
      "source": [
        "## 3. Embedding and Semantic Search\n",
        "Generates semantic embeddings and retrieves relevant documents using ChromaDB.\n",
        "### Embedding Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0HIIsd3pfA_n"
      },
      "outputs": [],
      "source": [
        "client = PersistentClient(path=CHROMA_PATH)\n",
        "embedding_function = SentenceTransformerEmbeddingFunction(model_name=\"all-MiniLM-L6-v2\")\n",
        "collection = client.get_or_create_collection(name='Insurance', embedding_function=embedding_function)\n",
        "cache_collection = client.get_or_create_collection(\n",
        "    name='Insurance_Cache',\n",
        "    embedding_function=embedding_function\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYmeQ8hbfEnS"
      },
      "source": [
        "### Adding Documents to Collection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "39mHhw8AfE-c"
      },
      "outputs": [],
      "source": [
        "collection.add(\n",
        "    documents=insurance_pdfs_data[\"Page_Text\"].tolist(),\n",
        "    ids=[str(i) for i in range(len(insurance_pdfs_data))],\n",
        "    metadatas=insurance_pdfs_data['Metadata'].tolist()\n",
        ")\n",
        "print(\"Documents successfully added to ChromaDB collection.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9sofTpMqfJ9z"
      },
      "source": [
        "### Querying with Caching"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oYBaRLajfKrN"
      },
      "outputs": [],
      "source": [
        "def retrieve_documents(query, n_results=5, cache_threshold=0.2):\n",
        "    \"\"\"\n",
        "    Retrieve top documents using semantic search with caching.\n",
        "\n",
        "    Parameters:\n",
        "    - query (str): User query.\n",
        "    - n_results (int): Number of results to retrieve.\n",
        "    - cache_threshold (float): Threshold for using cache.\n",
        "\n",
        "    Returns:\n",
        "    - DataFrame: Top documents with metadata.\n",
        "    \"\"\"\n",
        "\n",
        "    cache_results = cache_collection.query(query_texts=[query], n_results=1)\n",
        "    if cache_results['distances'][0] and cache_results['distances'][0][0] <= cache_threshold:\n",
        "        print(\"Cache hit for query:\", query)\n",
        "        cache_data = cache_results['metadatas'][0][0]\n",
        "        print(\"Cache retrieved data:\", cache_data)\n",
        "\n",
        "        documents = ast.literal_eval(cache_data['documents'])\n",
        "        distances = ast.literal_eval(cache_data['distances'])\n",
        "        metadatas = ast.literal_eval(cache_data['metadatas'])\n",
        "\n",
        "        return pd.DataFrame({\n",
        "            \"Documents\": documents,\n",
        "            \"Metadata\": metadatas,\n",
        "            \"Distances\": distances\n",
        "        })\n",
        "\n",
        "    print(\"Cache miss. Retrieving from main collection.\")\n",
        "    results = collection.query(query_texts=[query], n_results=n_results)\n",
        "    cache_collection.add(\n",
        "        documents=[query],\n",
        "        ids=[query],\n",
        "        metadatas=[{\n",
        "            \"documents\": str(results['documents'][0]),\n",
        "            \"distances\": str(results['distances'][0]),\n",
        "            \"metadatas\": str(results['metadatas'][0])\n",
        "        }]\n",
        "    )\n",
        "    print(\"Query added to cache.\")\n",
        "    print(\"Retrieved data from collection:\", results)\n",
        "    return pd.DataFrame({\n",
        "        \"Documents\": results['documents'][0],\n",
        "        \"Metadata\": results['metadatas'][0],\n",
        "        \"Distances\": results['distances'][0]\n",
        "    })"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-f4urBcffTTM"
      },
      "source": [
        "## 4. Cross-Encoder Reranking\n",
        "Applies cross-encoder models to refine document ranking based on relevance to the query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Ds7xTYVfV0W"
      },
      "outputs": [],
      "source": [
        "cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
        "\n",
        "def rerank_results(query, results_df):\n",
        "    \"\"\"\n",
        "    Rerank results using a cross-encoder.\n",
        "\n",
        "    Parameters:\n",
        "    - query (str): User query.\n",
        "    - results_df (DataFrame): Retrieved documents.\n",
        "\n",
        "    Returns:\n",
        "    - DataFrame: Reranked results.\n",
        "    \"\"\"\n",
        "    inputs = [[query, doc] for doc in results_df['Documents']]\n",
        "    scores = cross_encoder.predict(inputs)\n",
        "    results_df['Reranked_Scores'] = scores\n",
        "    print(\"Reranking complete. Top scores:\")\n",
        "    print(scores[:3])\n",
        "    return results_df.sort_values(by='Reranked_Scores', ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L96fNKMmfZB3"
      },
      "source": [
        "## 5. Response Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cw121QA5fadd"
      },
      "outputs": [],
      "source": [
        "def generate_response(query, top_docs):\n",
        "    \"\"\"\n",
        "    Generate a response using your required model.\n",
        "\n",
        "    Parameters:\n",
        "    - query (str): User query.\n",
        "    - top_docs (DataFrame): Retrieved documents.\n",
        "\n",
        "    Returns:\n",
        "    - str: Generated response.\n",
        "    \"\"\"\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": (\n",
        "                \"You are a helpful assistant specializing in insurance who can effectively answer user queries about insurance policies and documents. \"\n",
        "                \"You should provide clear, accurate, and concise answers directly addressing the user's query.\"\n",
        "            )\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": (\n",
        "                f\"\"\"\n",
        "                A user has asked the following question: '{query}'.\n",
        "\n",
        "                You also have access to search results from insurance policy documents stored in a DataFrame called '{top_docs}'.\n",
        "                - The column 'Documents' contains text extracted from insurance policy pages.\n",
        "                - The column 'Metadata' contains the policy name and page number as citations.\n",
        "\n",
        "                Use the following guidelines to formulate your response:\n",
        "                1. Provide accurate, relevant numbers or details from the documents, if available.\n",
        "                2. Summarize or restructure tables present in the text into a tabular format for clarity.\n",
        "                3. Use only the information relevant to the query and ignore irrelevant details.\n",
        "                4. Cite the policy name and page number from the 'Metadata' column to support your answer.\n",
        "                5. If you cannot provide a complete answer, suggest sections or topics to help the user locate relevant information in the cited documents.\n",
        "                6. Do not disclose internal workings or mention limitations; respond as a customer-facing assistant.\n",
        "\n",
        "                Format the final response as clear, user-friendly text followed by properly formatted citations. If the query is irrelevant, state so clearly.\n",
        "                \"\"\"\n",
        "            )\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"assistant\",\n",
        "            \"content\": (\n",
        "                \"Use the top 3 relevant documents to address the user's question directly and cite the relevant policies and pages. \"\n",
        "                \"Ensure the response is easy to read and formatted clearly for the user.\"\n",
        "            )\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    # Generate the response using the OpenAI API\n",
        "    response = openai.chat.completions.create(model=MODEL_NAME, messages=messages)\n",
        "\n",
        "    print(\"Generated response:\")\n",
        "    print(response.choices[0].message.content)\n",
        "\n",
        "    return response.choices[0].message.content\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ls9Tr6MsfPcn"
      },
      "source": [
        "## 6. Interactive Query, Reranking, and Visualization\n",
        "This section integrates querying, reranking, and visualization for an end-to-end user workflow. This can be used to test out the model.\n",
        "\n",
        "\n",
        "### Integrated Workflow with Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E1Sf-9BEfQSe"
      },
      "outputs": [],
      "source": [
        "query_input = widgets.Text(description=\"Query:\", placeholder=\"Enter your query...\")\n",
        "submit_button = widgets.Button(description=\"Submit\")\n",
        "output_area = widgets.Output()\n",
        "\n",
        "def visualize_relevance(df):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.bar(df['Metadata'].apply(lambda x: x['Page_No']), df['Distances'], color='skyblue')\n",
        "    plt.xlabel(\"Page No.\")\n",
        "    plt.ylabel(\"Relevance Score\")\n",
        "    plt.title(\"Relevance of Retrieved Documents\")\n",
        "    plt.show()\n",
        "\n",
        "def visualize_reranking(df):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.bar(df['Metadata'].apply(lambda x: x['Page_No']), df['Reranked_Scores'], color='orange')\n",
        "    plt.xlabel(\"Page No.\")\n",
        "    plt.ylabel(\"Reranked Score\")\n",
        "    plt.title(\"Reranked Relevance of Documents\")\n",
        "    plt.show()\n",
        "\n",
        "def integrated_workflow(query):\n",
        "    df = retrieve_documents(query)\n",
        "    print(\"Top retrieved documents (before reranking):\")\n",
        "    print(df[['Documents', 'Metadata']])\n",
        "    visualize_relevance(df)\n",
        "\n",
        "    # Rerank documents\n",
        "    reranked_df = rerank_results(query, df)\n",
        "    print(\"Top reranked documents:\")\n",
        "    print(reranked_df[['Documents', 'Metadata', 'Reranked_Scores']])\n",
        "    visualize_reranking(reranked_df)\n",
        "\n",
        "    # Generate response\n",
        "    top_docs = reranked_df.head(3)  # Taking top 3 documents after reranking\n",
        "    response = generate_response(query, top_docs)\n",
        "    print(\"Generated Response:\")\n",
        "    print(response)\n",
        "\n",
        "    return reranked_df, response\n",
        "\n",
        "def on_submit(b):\n",
        "    with output_area:\n",
        "        output_area.clear_output()\n",
        "        query = query_input.value\n",
        "        print(f\"Processing query: {query}\")\n",
        "        reranked_df, response = integrated_workflow(query)\n",
        "\n",
        "submit_button.on_click(on_submit)\n",
        "display(query_input, submit_button, output_area)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5S3w8RgfegN"
      },
      "source": [
        "\n",
        "## 7. Testing with Multiple Queries\n",
        "Checking the system's functionality with predefined test queries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hWivZNZEfir5"
      },
      "outputs": [],
      "source": [
        "# Pre-defined test queries\n",
        "queries = [\n",
        "    \"Do all the insurance policies cover diabetic patients?\",\n",
        "    \"What is the average premium rate for individuals above 60 years of age?\",\n",
        "    \"Is there a waiting period for pre-existing conditions for the policies?\"\n",
        "]\n",
        "\n",
        "for idx, test_query in enumerate(queries, start=1):\n",
        "    print(f\"\\nRunning test query {idx}: {test_query}\")\n",
        "    _, test_response = integrated_workflow(test_query)\n",
        "    print(f\"Response for Query {idx}:\\n{test_response}\\n\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}